{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9112d8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3457ffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Definindo o número de classes que serão previstas\n",
    "num_classes = 4\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = Sequential()\n",
    "# Adicionando uma camada de entrada com 3 neurônios, uma camada oculta com 4 e as 4 classes de saída\n",
    "modelo.add(Dense(3, input_dim=3, activation='relu'))\n",
    "modelo.add(Dense(4, activation='relu'))\n",
    "modelo.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compilando o modelo\n",
    "modelo.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b105e87f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>linha</th>\n",
       "      <th>qPA</th>\n",
       "      <th>pulso</th>\n",
       "      <th>resp</th>\n",
       "      <th>gravid</th>\n",
       "      <th>classe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>8.416754</td>\n",
       "      <td>75.921057</td>\n",
       "      <td>21.635259</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>8.725890</td>\n",
       "      <td>63.813564</td>\n",
       "      <td>19.718734</td>\n",
       "      <td>41.530427</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>197.210213</td>\n",
       "      <td>19.045471</td>\n",
       "      <td>52.730745</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>8.733333</td>\n",
       "      <td>143.636181</td>\n",
       "      <td>17.621141</td>\n",
       "      <td>34.679911</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.747626</td>\n",
       "      <td>82.636672</td>\n",
       "      <td>12.209535</td>\n",
       "      <td>69.375882</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>1496</td>\n",
       "      <td>4.774128</td>\n",
       "      <td>6.701052</td>\n",
       "      <td>7.380529</td>\n",
       "      <td>24.494467</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>1497</td>\n",
       "      <td>8.400144</td>\n",
       "      <td>81.203081</td>\n",
       "      <td>13.630509</td>\n",
       "      <td>65.385011</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>1498</td>\n",
       "      <td>4.768427</td>\n",
       "      <td>143.261527</td>\n",
       "      <td>21.843486</td>\n",
       "      <td>24.530704</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>1499</td>\n",
       "      <td>1.707746</td>\n",
       "      <td>184.267283</td>\n",
       "      <td>14.256125</td>\n",
       "      <td>61.062546</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>1500</td>\n",
       "      <td>4.745181</td>\n",
       "      <td>196.803149</td>\n",
       "      <td>11.875068</td>\n",
       "      <td>39.110506</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      linha       qPA       pulso       resp     gravid  classe\n",
       "0         1  8.416754   75.921057  21.635259  40.000000       2\n",
       "1         2  8.725890   63.813564  19.718734  41.530427       2\n",
       "2         3  0.000000  197.210213  19.045471  52.730745       3\n",
       "3         4  8.733333  143.636181  17.621141  34.679911       2\n",
       "4         5  1.747626   82.636672  12.209535  69.375882       3\n",
       "...     ...       ...         ...        ...        ...     ...\n",
       "1495   1496  4.774128    6.701052   7.380529  24.494467       1\n",
       "1496   1497  8.400144   81.203081  13.630509  65.385011       3\n",
       "1497   1498  4.768427  143.261527  21.843486  24.530704       1\n",
       "1498   1499  1.707746  184.267283  14.256125  61.062546       3\n",
       "1499   1500  4.745181  196.803149  11.875068  39.110506       2\n",
       "\n",
       "[1500 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('treino_sinais_vitais_com_label.txt', header=None,\n",
    "                 usecols=[0,3, 4, 5, 6, 7])\n",
    "df = df.rename(columns={0: 'linha',\n",
    "                        3: 'qPA',\n",
    "                        4: 'pulso',\n",
    "                        5: 'resp',\n",
    "                        6: 'gravid',\n",
    "                        7: 'classe'})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c824db73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#Divisao de testes e treino\n",
    "x_class = df[['qPA', 'pulso', 'resp']]\n",
    "y_class = df['classe']\n",
    "        \n",
    "xc_train, xc_test, yc_train, yc_test = train_test_split(\n",
    "    x_class, y_class, test_size=0.2, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37f8ae67",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'int'>\"})",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m entradas \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(x_class)\n\u001b[0;32m      3\u001b[0m saida \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m]\n\u001b[1;32m----> 5\u001b[0m \u001b[43mmodelo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentradas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaida\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1134\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1128\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cluster_coordinator \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mcoordinator\u001b[38;5;241m.\u001b[39mClusterCoordinator(\n\u001b[0;32m   1129\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mscope(), \\\n\u001b[0;32m   1132\u001b[0m      training_utils\u001b[38;5;241m.\u001b[39mRespectCompiledTrainableState(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1133\u001b[0m   \u001b[38;5;66;03m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[39;00m\n\u001b[1;32m-> 1134\u001b[0m   data_handler \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1135\u001b[0m \u001b[43m      \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1136\u001b[0m \u001b[43m      \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1137\u001b[0m \u001b[43m      \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1138\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[43m      \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1140\u001b[0m \u001b[43m      \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1141\u001b[0m \u001b[43m      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1142\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m      \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m      \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m      \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m      \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1150\u001b[0m   \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   1151\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py:1383\u001b[0m, in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cluster_coordinator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1382\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py:1137\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1134\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution \u001b[38;5;241m=\u001b[39m steps_per_execution\n\u001b[0;32m   1135\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution_value \u001b[38;5;241m=\u001b[39m steps_per_execution\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m-> 1137\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m \u001b[43mselect_data_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m adapter_cls(\n\u001b[0;32m   1139\u001b[0m     x,\n\u001b[0;32m   1140\u001b[0m     y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1149\u001b[0m     distribution_strategy\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy(),\n\u001b[0;32m   1150\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   1152\u001b[0m strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py:976\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    973\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mcls\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ALL_ADAPTER_CLS \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mcan_handle(x, y)]\n\u001b[0;32m    974\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m adapter_cls:\n\u001b[0;32m    975\u001b[0m   \u001b[38;5;66;03m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[1;32m--> 976\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    977\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to find data adapter that can handle \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    978\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    979\u001b[0m           _type_name(x), _type_name(y)))\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(adapter_cls) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    981\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    982\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData adapters should be mutually exclusive for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    983\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhandling inputs. Found multiple adapters \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m to handle \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    984\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    985\u001b[0m           adapter_cls, _type_name(x), _type_name(y)))\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'int'>\"})"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "entradas = np.array(x_class)\n",
    "saida = [1,2,3,4]\n",
    "\n",
    "modelo.fit(entradas, saida, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a15a587",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d2eece",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('treino_sinais_vitais_com_label.txt', header=None,\n",
    "                 usecols=[0,3, 4, 5, 6, 7])\n",
    "df = df.rename(columns={0: 'linha',\n",
    "                        3: 'qPA',\n",
    "                        4: 'pulso',\n",
    "                        5: 'resp',\n",
    "                        6: 'gravid',\n",
    "                        7: 'classe'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb405f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divisao de testes e treino\n",
    "x_class = df[['qPA', 'pulso', 'resp']]\n",
    "y_class = df['classe']\n",
    "        \n",
    "xc_train, xc_test, yc_train, yc_test = train_test_split(\n",
    "    x_class, y_class, test_size=TEST_SIZE, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5771160a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Modelagem\n",
    "modelo = RandomForestClassifier(random_state=RANDOM_STATE, max_depth=MAX_DEPTH)\n",
    "modelo.fit(xc_train, yc_train)\n",
    "\n",
    "y_class_pred = modelo.predict(xc_test)\n",
    "accuracy = accuracy_score(yc_test, y_class_pred)\n",
    "\n",
    "print('Acurácia:', accuracy)\n",
    "print('F1-score: ', f1_score(yc_test, y_class_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cfeb95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f952f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def training(label=0):\n",
    "    base = reader(label)\n",
    "    vetor = []\n",
    "\n",
    "    for b in base:\n",
    "        values = np.ravel(list(b.values()))\n",
    "        vetor.append(values)\n",
    "\n",
    "    def classification():\n",
    "        df = pd.DataFrame(vetor, columns=['qPA', 'bpm', 'fpm', 'gravidade'])\n",
    "\n",
    "        df['gravidade'] = pd.cut(df['gravidade'], bins=[0, 25, 50, 75, 100],\n",
    "                                 labels=['1', '2', '3', '4'], include_lowest=True)\n",
    "\n",
    "        X = df[['qPA', 'bpm', 'fpm']]\n",
    "        y = df['gravidade']\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "\n",
    "        modelo = RandomForestClassifier(n_estimators=ESTIMATORS, max_depth=DEPTH, random_state=RANDOM_STATE)\n",
    "        modelo.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = modelo.predict(X_test)\n",
    "        report = classification_report(y_test, y_pred, zero_division=1)\n",
    "\n",
    "        print('\\n\\n--CLASSIFIER--\\n')\n",
    "\n",
    "        print(report)\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        cmd = ConfusionMatrixDisplay(\n",
    "            cm, display_labels=['Crítico', 'Instável', 'p. Estável', 'Estável'])\n",
    "        cmd.plot()\n",
    "        plt.savefig('random_forest-matriz_confusão.png')\n",
    "        \n",
    "        def predict_gravidade(X):\n",
    "            test = pd.DataFrame(X)\n",
    "            test.columns = ['qPA', 'bpm', 'fpm']\n",
    "            classes = modelo.predict(test)\n",
    "            return classes[0]\n",
    "\n",
    "        return predict_gravidade\n",
    "\n",
    "    def regression():\n",
    "        df = pd.DataFrame(vetor, columns=['qPA', 'bpm', 'fpm', 'gravidade'])\n",
    "        X = df[['qPA', 'bpm', 'fpm']]\n",
    "        y = df['gravidade']\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "\n",
    "        modelo = RandomForestRegressor(n_estimators=ESTIMATORS, max_depth=DEPTH, random_state=RANDOM_STATE)\n",
    "        modelo.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = modelo.predict(X_test)\n",
    "\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        evs = explained_variance_score(y_test, y_pred)\n",
    "        max_err = max_error(y_test, y_pred)\n",
    "\n",
    "        print('\\n--REGRESSÃO--\\n')\n",
    "\n",
    "        print(f\"MSE: {mse:.4f}\")\n",
    "        print(f\"MAE: {mae:.4f}\")\n",
    "        print(f\"R²: {r2:.4f}\")\n",
    "        print(f\"Explained Variance Score: {evs:.4f}\")\n",
    "        print(f\"Max Error: {max_err:.4f}\")\n",
    "\n",
    "        def predict_gravidade(X):\n",
    "            test = pd.DataFrame(X)\n",
    "            test.columns = ['qPA', 'bpm', 'fpm']\n",
    "            gravidade = modelo.predict(test)\n",
    "            return gravidade[0]\n",
    "\n",
    "        return predict_gravidade\n",
    "\n",
    "    return regression(), classification()\n",
    "\n",
    "\n",
    "def randomForest(samples):\n",
    "\n",
    "    reg, cla = training()\n",
    "\n",
    "    samples = np.array(samples)\n",
    "    response = []\n",
    "    i = 0\n",
    "\n",
    "    for sample in samples:\n",
    "        gravidade = reg(sample.reshape(1, -1))\n",
    "        classe = cla(sample.reshape(1, -1))\n",
    "        response.append([i, gravidade, classe])\n",
    "        i += 1\n",
    "\n",
    "    with open('random_forest-test_response.txt', 'w') as f:\n",
    "        for r in response:\n",
    "            f.write(f'{r[0]}, {r[1]}, {r[2]}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
